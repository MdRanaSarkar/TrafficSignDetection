{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2437fb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50e3a55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6c8d132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations. To begin with, we shall keep it minimum - only resizing the images and converting them to PyTorch tensors\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize([112, 112]),\n",
    "    transforms.ToTensor()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a983dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining hyperparameters\n",
    "BATCH_SIZE = 256\n",
    "learning_rate = 0.001\n",
    "EPOCHS = 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c36903c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples = 27839\n",
      "Number of validation samples = 6960\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define path of training data\n",
    "\n",
    "train_data_path = \"C:\\\\Users\\\\HDSL53\\\\Desktop\\\\HawarITMLProjects\\\\With_GTSRB_datasets\\\\myData\"\n",
    "train_data = torchvision.datasets.ImageFolder(root = train_data_path, transform = data_transforms)\n",
    "\n",
    "# Divide data into training and validation (0.8 and 0.2)\n",
    "ratio = 0.8\n",
    "n_train_examples = int(len(train_data) * ratio)\n",
    "n_val_examples = len(train_data) - n_train_examples\n",
    "\n",
    "train_data, val_data = data.random_split(train_data, [n_train_examples, n_val_examples])\n",
    "\n",
    "print(f\"Number of training samples = {len(train_data)}\")\n",
    "print(f\"Number of validation samples = {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc98e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories\n",
    "import pathlib\n",
    "root=pathlib.Path(train_data_path)\n",
    "classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eefcd30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numClasses=len(classes)\n",
    "numClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8c925a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot histogram for training and validation data\n",
    "train_hist = [0]*numClasses\n",
    "for i in train_data.indices:\n",
    "    tar = train_data.dataset.targets[i]\n",
    "    train_hist[tar] += 1\n",
    "    \n",
    "val_hist = [0]*numClasses\n",
    "for i in val_data.indices:\n",
    "    tar = val_data.dataset.targets[i]\n",
    "    val_hist[tar] += 1\n",
    "\n",
    "plt.bar(range(numClasses), train_hist, label=\"train\")\n",
    "plt.bar(range(numClasses), val_hist, label=\"val\")\n",
    "legend = plt.legend(loc='upper right', shadow=True)\n",
    "plt.title(\"Distribution Plot\")\n",
    "plt.xlabel(\"Class ID\")\n",
    "plt.ylabel(\"# of examples\")\n",
    "\n",
    "plt.savefig(\"train_val_split.png\", bbox_inches = 'tight', pad_inches=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa0a6bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader for training and validation\n",
    "\n",
    "train_loader = data.DataLoader(train_data, shuffle=True, batch_size = BATCH_SIZE)\n",
    "val_loader = data.DataLoader(val_data, shuffle=True, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e410c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count the number of parameters in the model\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b776e320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class AlexnetTS(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(in_channels=64, out_channels=192, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(in_channels=192, out_channels=384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256*7*7, 1000),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=1000, out_features=256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Linear(256, output_dim)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        h = x.view(x.shape[0], -1)\n",
    "        x = self.classifier(h)\n",
    "        return x, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a527c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb0b49b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 15,064,148 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "# The model is defined in the class AlexnetTS in the file class_alexnetTS.py\n",
    "model = AlexnetTS(numClasses)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0c6a5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and criterion functions\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65a8c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If CUDA is available, convert model and loss to cuda variables\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a915affe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexnetTS(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (12): ReLU(inplace=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=12544, out_features=1000, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=1000, out_features=256, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=256, out_features=44, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c6812eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 56, 56]           1,792\n",
      "         MaxPool2d-2           [-1, 64, 28, 28]               0\n",
      "              ReLU-3           [-1, 64, 28, 28]               0\n",
      "            Conv2d-4          [-1, 192, 28, 28]         110,784\n",
      "         MaxPool2d-5          [-1, 192, 14, 14]               0\n",
      "              ReLU-6          [-1, 192, 14, 14]               0\n",
      "            Conv2d-7          [-1, 384, 14, 14]         663,936\n",
      "              ReLU-8          [-1, 384, 14, 14]               0\n",
      "            Conv2d-9          [-1, 256, 14, 14]         884,992\n",
      "             ReLU-10          [-1, 256, 14, 14]               0\n",
      "           Conv2d-11          [-1, 256, 14, 14]         590,080\n",
      "        MaxPool2d-12            [-1, 256, 7, 7]               0\n",
      "             ReLU-13            [-1, 256, 7, 7]               0\n",
      "          Dropout-14                [-1, 12544]               0\n",
      "           Linear-15                 [-1, 1000]      12,545,000\n",
      "             ReLU-16                 [-1, 1000]               0\n",
      "          Dropout-17                 [-1, 1000]               0\n",
      "           Linear-18                  [-1, 256]         256,256\n",
      "             ReLU-19                  [-1, 256]               0\n",
      "           Linear-20                   [-1, 44]          11,308\n",
      "================================================================\n",
      "Total params: 15,064,148\n",
      "Trainable params: 15,064,148\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.14\n",
      "Forward/backward pass size (MB): 6.63\n",
      "Params size (MB): 57.47\n",
      "Estimated Total Size (MB): 64.24\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HDSL53\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "print(summary(model, (3, 112, 112)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea11c23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state dict:\n",
      "features.0.weight \t torch.Size([64, 3, 3, 3])\n",
      "features.0.bias \t torch.Size([64])\n",
      "features.3.weight \t torch.Size([192, 64, 3, 3])\n",
      "features.3.bias \t torch.Size([192])\n",
      "features.6.weight \t torch.Size([384, 192, 3, 3])\n",
      "features.6.bias \t torch.Size([384])\n",
      "features.8.weight \t torch.Size([256, 384, 3, 3])\n",
      "features.8.bias \t torch.Size([256])\n",
      "features.10.weight \t torch.Size([256, 256, 3, 3])\n",
      "features.10.bias \t torch.Size([256])\n",
      "classifier.1.weight \t torch.Size([1000, 12544])\n",
      "classifier.1.bias \t torch.Size([1000])\n",
      "classifier.4.weight \t torch.Size([256, 1000])\n",
      "classifier.4.bias \t torch.Size([256])\n",
      "classifier.6.weight \t torch.Size([44, 256])\n",
      "classifier.6.bias \t torch.Size([44])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print model's state dict\n",
    "\n",
    "print(\"Model's state dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7dafcd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer details:\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print optimizer details\n",
    "\n",
    "print(\"Optimizer details:\")\n",
    "print(optimizer)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "339a3721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate accuracy\n",
    "\n",
    "def calculate_accuracy(y_pred, y):\n",
    "    top_pred = y_pred.argmax(1, keepdim = True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76f075fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform training of the model\n",
    "\n",
    "def train(model, loader, opt, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    # Train the model\n",
    "    model.train()\n",
    "    \n",
    "    for (images, labels) in loader:\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "        \n",
    "        images = images\n",
    "        labels = labels\n",
    "        # Training pass\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        output, _ = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        acc = calculate_accuracy(output, labels)\n",
    "        \n",
    "        # Optimizing weights\n",
    "        opt.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74b18123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform evaluation on the trained model\n",
    "\n",
    "def evaluate(model, loader, opt, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for (images, labels) in loader:\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "                \n",
    "            images = images\n",
    "            labels = labels\n",
    "            \n",
    "            # Run predictions\n",
    "            output, _ = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            acc = calculate_accuracy(output, labels)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d5591a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-0: \n"
     ]
    }
   ],
   "source": [
    "# Perform training\n",
    "\n",
    "# List to save training and val loss and accuracies\n",
    "train_loss_list = [0]*EPOCHS\n",
    "train_acc_list = [0]*EPOCHS\n",
    "val_loss_list = [0]*EPOCHS\n",
    "val_acc_list = [0]*EPOCHS\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"Epoch-%d: \" % (epoch))\n",
    "\n",
    "    train_start_time = time.monotonic()\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    train_end_time = time.monotonic()\n",
    "\n",
    "    val_start_time = time.monotonic()\n",
    "    val_loss, val_acc = evaluate(model, val_loader, optimizer, criterion)\n",
    "    val_end_time = time.monotonic()\n",
    "    \n",
    "    train_loss_list[epoch] = train_loss\n",
    "    train_acc_list[epoch] = train_acc\n",
    "    val_loss_list[epoch] = val_loss\n",
    "    val_acc_list[epoch] = val_acc\n",
    "    \n",
    "    print(\"Training: Loss = %.4f, Accuracy = %.4f, Time = %.2f seconds\" % (train_loss, train_acc, train_end_time - train_start_time))\n",
    "    print(\"Validation: Loss = %.4f, Accuracy = %.4f, Time = %.2f seconds\" % (val_loss, val_acc, val_end_time - val_start_time))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c7a74f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
